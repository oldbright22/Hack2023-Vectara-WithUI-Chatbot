{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìé Embedding HTML Docs into Pinecone using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxx\n",
      "7440d145-170c-4b35-9448-249e92d4dc94\n",
      "None\n",
      "langchain-retrieval\n"
     ]
    }
   ],
   "source": [
    "OPENAI_API_KEY = \"xxx\"\n",
    "PINECONE_API_KEY = \"yyy\"\n",
    "PINECONE_ENV = \"gcp-starter\"\n",
    "PINECONE_INDEX = \"langchain-retrieval\"\n",
    "\n",
    "#OPENAI_API_KEY = os.getenv(\"openai_api_key\")\n",
    "#PINECONE_API_KEY = os.getenv(\"pinecone_api_key\")\n",
    "#PINECONE_ENV_KEY = os.getenv(\"pinecone_env_key\")\n",
    "#PINECONE_INDEX = os.getenv(\"pinecone_index\")\n",
    "\n",
    "print(OPENAI_API_KEY)\n",
    "print(PINECONE_API_KEY)\n",
    "print(PINECONE_ENV_KEY)\n",
    "print(PINECONE_INDEX)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Decathlon Documents from the Decathlon website (Home, FAQ, Warranty, Product Returns etc...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loader = WebBaseLoader([\"https://www.decathlon.ma/\", \"https://www.decathlon-united.com/fr\", \"https://www.decathlon.ma/content/63-garantie\", \"https://www.decathlon.ma/content/87-retour-echange\", \n",
    "#                        \"https://www.decathlon.ma/content/88-cartecadeaux\", \"https://www.decathlon.ma/content/96-cliquez-et-retirez\", \"https://www.decathlon.ma/content/85-echo-conception\",\n",
    "#                        \"https://www.decathlon.ma/content/86-nos-innovations\", \"https://www.decathlon.ma/module/decab2b/b2b?icn=HomePage-Footer-DecathlonPRO\", \"https://www.decathlon.ma/page/acheter-en-ligne.html\", \n",
    "#                        \"https://www.decathlon.ma/page/consulter-stock.html\", \"https://www.decathlon.ma/content/1-livraison\", \"https://www.decathlon.ma/page/rappelproduit.html\", \"https://www.decathlon.ma/page/cgu_cgv.html\"\n",
    "#                        \"https://www.decathlon.ma/page/donnees-personnelles-et-cookies.html\", \"https://www.decathlon.ma/page/conditions-de-publication-des-avis.html\", \"https://www.decathlon.ma/page/mention_legale.html\",\n",
    "#                        \"https://www.decathlon.ma/content/102-decathlon-occasion?icn=ServicesPage-occasion\", \"https://www.decathlon.ma/5080-promotions?icn=HomePage-Menu-Promotions\", \"https://www.decathlon.ma/nous-contacter\"])\n",
    "\n",
    "loader = WebBaseLoader([\"https://www.amazon.com/\",\"https://www.amazon.com/AmazonBasics-Volleyball-Badminton-Combo-Set/product-reviews/B07GXS216T\", \"https://www.amazon.com/AmazonBasics-Ladder-Toss-Outdoor-Carrying/product-reviews/B0145IWKBE\"])\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get 48 Documents after loading the HTML Files using Langchain WebBaseLoader and then splittingn them"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to embedd the documents and then store them into a Pincecone Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "import pinecone\n",
    "\n",
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,\n",
    "    environment=PINECONE_ENV_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name=PINECONE_INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.00099,\n",
       " 'namespaces': {'': {'vector_count': 99}},\n",
       " 'total_vector_count': 99}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pinecone.Index(index_name)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing the docs into the Pinecone index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_store = Pinecone.from_texts([d.page_content for d in docs], embedding, index_name=index_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚õìÔ∏è Langchain LLM Chatbot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use gpt-3.5 OpenAI model to create a chatbot that can answer customers questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_from_query(db, query, k=4):\n",
    "        \"\"\"\n",
    "        Function that generates a response to a customer question using the gpt-3.5-model and the docs provided\n",
    "        \"\"\"\n",
    "\n",
    "        docs = db.similarity_search(query, k=k)\n",
    "        docs_page_content = \" \".join([d.page_content for d in docs])\n",
    "\n",
    "        chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "        # Template to use for the system message prompt\n",
    "        template = \"\"\"\n",
    "            I am an assistant designed to answer customer queries on an e-commerce platform that sells sports equipment {docs}.\n",
    "            I also function as a chatbot, responding to user phrases like \"Thank you\", \"Hello\", etc.\n",
    "            First, I will classify the sentiment of the customer's question or statement.\n",
    "            I will use only the given information to answer the question, considering the sentiment of the customer's input.\n",
    "            If I lack the necessary information or can't find a suitable answer, I will respond with I'm sorry I do not have this answer.\"\n",
    "            If the input isn't a question, I will act as a chatbot assisting customers.\n",
    "            My answers will be brief yet detailed.\n",
    "\n",
    "            Please go ahead with your query or statement related to sport equipment or any other greetings, and I will respond accordingly!\n",
    "            \"\"\"\n",
    "\n",
    "        system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "\n",
    "        # Human question prompt\n",
    "        human_template = \" Please provide the customer's input, and I'll respond accordingly : {question}\"\n",
    "        human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "        chat_prompt = ChatPromptTemplate.from_messages(\n",
    "            [system_message_prompt, human_message_prompt]\n",
    "        )\n",
    "\n",
    "        chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
    "\n",
    "        try:\n",
    "            response = chain.run(question=query, docs=docs_page_content)\n",
    "            response = response.replace(\"\\n\", \"\")\n",
    "            return response\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "response = get_response_from_query(doc_store, \"what are featured recommendations?\")\n",
    "response\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the already created Index to query the db and generate a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = Pinecone.from_existing_index(index_name, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "response = get_response_from_query(docsearch, \"what are featured recommendations?\")\n",
    "response\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
