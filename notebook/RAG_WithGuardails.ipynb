{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dac002",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementing RAG with Guardrails\n",
    "\n",
    "## To implement RAG with guardrails, we will rely on the NVIDIA NeMo Guardrails library. \n",
    "## The library primarily focuses on AI safety by implementing \"guardrails\" as protective measures against unwanted interactions.\n",
    "## However, we can also use these guardrails to trigger things like RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d138da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU \\\n",
    "    nemoguardrails==0.4.0 \\\n",
    "    pinecone-client==2.2.2 \\\n",
    "    datasets==2.14.3 \\\n",
    "    openai==0.27.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c3be97a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in c:\\users\\btina\\appdata\\roaming\\python\\python310\\site-packages (2.14.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\miniconda3\\lib\\site-packages (from datasets) (1.25.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\btina\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (14.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\btina\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in c:\\users\\btina\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (2.1.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\btina\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from datasets) (4.65.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\btina\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\btina\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in c:\\users\\btina\\appdata\\roaming\\python\\python310\\site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\btina\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in c:\\users\\btina\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (0.17.3)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\miniconda3\\lib\\site-packages (from datasets) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\btina\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\btina\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\btina\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\btina\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\btina\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\miniconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\btina\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2023.5.7)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\miniconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\miniconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\btina\\appdata\\roaming\\python\\python310\\site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\btina\\appdata\\roaming\\python\\python310\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\miniconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "## Building the Knowledge Base\n",
    "%pip install -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dd6b5a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset_builder\n",
    "\n",
    "ds_builder = load_dataset_builder(\"jamescalam/llama-2-arxiv-papers-chunked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e51fd1a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doi': Value(dtype='string', id=None),\n",
       " 'chunk-id': Value(dtype='string', id=None),\n",
       " 'chunk': Value(dtype='string', id=None),\n",
       " 'id': Value(dtype='string', id=None),\n",
       " 'title': Value(dtype='string', id=None),\n",
       " 'summary': Value(dtype='string', id=None),\n",
       " 'source': Value(dtype='string', id=None),\n",
       " 'authors': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'categories': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'comment': Value(dtype='string', id=None),\n",
       " 'journal_ref': Value(dtype='string', id=None),\n",
       " 'primary_category': Value(dtype='string', id=None),\n",
       " 'published': Value(dtype='string', id=None),\n",
       " 'updated': Value(dtype='string', id=None),\n",
       " 'references': [{'id': Value(dtype='string', id=None),\n",
       "   'title': Value(dtype='string', id=None),\n",
       "   'authors': Value(dtype='string', id=None),\n",
       "   'year': Value(dtype='string', id=None)}]}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_builder.info.description\n",
    "ds_builder.info.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4b2de213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "696b9d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\n",
    "    \"jamescalam/llama-2-arxiv-papers-chunked\",\n",
    "    split=\"train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dcc13392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doi': '1102.0183',\n",
       " 'chunk-id': '0',\n",
       " 'chunk': 'High-Performance Neural Networks\\nfor Visual Object Classi\\x0ccation\\nDan C. Cire\\x18 san, Ueli Meier, Jonathan Masci,\\nLuca M. Gambardella and J\\x7f urgen Schmidhuber\\nTechnical Report No. IDSIA-01-11\\nJanuary 2011\\nIDSIA / USI-SUPSI\\nDalle Molle Institute for Arti\\x0ccial Intelligence\\nGalleria 2, 6928 Manno, Switzerland\\nIDSIA is a joint institute of both University of Lugano (USI) and University of Applied Sciences of Southern Switzerland (SUPSI),\\nand was founded in 1988 by the Dalle Molle Foundation which promoted quality of life.\\nThis work was partially supported by the Swiss Commission for Technology and Innovation (CTI), Project n. 9688.1 IFF:\\nIntelligent Fill in Form.arXiv:1102.0183v1  [cs.AI]  1 Feb 2011\\nTechnical Report No. IDSIA-01-11 1\\nHigh-Performance Neural Networks\\nfor Visual Object Classi\\x0ccation\\nDan C. Cire\\x18 san, Ueli Meier, Jonathan Masci,\\nLuca M. Gambardella and J\\x7f urgen Schmidhuber\\nJanuary 2011\\nAbstract\\nWe present a fast, fully parameterizable GPU implementation of Convolutional Neural\\nNetwork variants. Our feature extractors are neither carefully designed nor pre-wired, but',\n",
       " 'id': '1102.0183',\n",
       " 'title': 'High-Performance Neural Networks for Visual Object Classification',\n",
       " 'summary': 'We present a fast, fully parameterizable GPU implementation of Convolutional\\nNeural Network variants. Our feature extractors are neither carefully designed\\nnor pre-wired, but rather learned in a supervised way. Our deep hierarchical\\narchitectures achieve the best published results on benchmarks for object\\nclassification (NORB, CIFAR10) and handwritten digit recognition (MNIST), with\\nerror rates of 2.53%, 19.51%, 0.35%, respectively. Deep nets trained by simple\\nback-propagation perform better than more shallow ones. Learning is\\nsurprisingly rapid. NORB is completely trained within five epochs. Test error\\nrates on MNIST drop to 2.42%, 0.97% and 0.48% after 1, 3 and 17 epochs,\\nrespectively.',\n",
       " 'source': 'http://arxiv.org/pdf/1102.0183',\n",
       " 'authors': ['Dan C. Cireşan',\n",
       "  'Ueli Meier',\n",
       "  'Jonathan Masci',\n",
       "  'Luca M. Gambardella',\n",
       "  'Jürgen Schmidhuber'],\n",
       " 'categories': ['cs.AI', 'cs.NE'],\n",
       " 'comment': '12 pages, 2 figures, 5 tables',\n",
       " 'journal_ref': None,\n",
       " 'primary_category': 'cs.AI',\n",
       " 'published': '20110201',\n",
       " 'updated': '20110201',\n",
       " 'references': []}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4642ddc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reformat the data to keep only what is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "60b05ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references', 'uid'],\n",
       "    num_rows: 4838\n",
       "})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## https://www.pinecone.io/learn/fast-retrieval-augmented-generation/\n",
    "\n",
    "data = data.map(lambda x: {\n",
    "    'uid': f\"{x['doi']}-{x['chunk-id']}\"\n",
    "})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d691b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The chunk field contains the text we will encode and store inside Pinecone\n",
    "\n",
    "## To encode that data, we need to use an embedding model\n",
    "\n",
    "## We will use OpenAI's text-embedding-ada-002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3add42bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9a7af422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://platform.openai.com/account/api-keys\n",
    "os.environ['OPENAI_API_KEY'] = \"zzz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "84929631",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7f53a605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "embed_model_id = \"text-embedding-ada-002\"\n",
    "\n",
    "res = openai.Embedding.create(\n",
    "    input=[\n",
    "        \"We would have some text to embed here\",\n",
    "        \"And maybe another chunk here too\"\n",
    "    ], engine=embed_model_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "165580c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1536, 1536)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res['data'][0]['embedding']), len(res['data'][1]['embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "96cfb50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "\n",
    "# initialize connection to pinecone (get API key at app.pinecone.io)\n",
    "api_key = \"xx\"\n",
    "# find your environment next to the api key in pinecone console\n",
    "env = \"gcp-starter\"\n",
    "\n",
    "pinecone.init(api_key=api_key, environment=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d8bf237e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "index_name = \"langchain-retrieval\"\n",
    "\n",
    "# check if index already exists (it shouldn't if this is first time)\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    # if does not exist, create index\n",
    "    pinecone.create_index(\n",
    "        index_name,\n",
    "        dimension=len(res['data'][0]['embedding']),\n",
    "        metric='cosine'\n",
    "    )\n",
    "    # wait for index to be initialized\n",
    "    while not pinecone.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)\n",
    "\n",
    "# connect to index\n",
    "index = pinecone.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cd0ed96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  RAG with Guardrails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d478d975",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def retrieve(query: str) -> list:\n",
    "    \n",
    "    # create query embedding\n",
    "    res = openai.Embedding.create(input=[query], engine=embed_model_id)\n",
    "    xq = res['data'][0]['embedding']\n",
    "    \n",
    "    # get relevant contexts from pinecone\n",
    "    res = index.query(xq, top_k=5, include_metadata=True)\n",
    "    \n",
    "    # get list of retrieved texts\n",
    "    contexts = [x['metadata']['chunk'] for x in res['matches']]\n",
    "    return contexts\n",
    "\n",
    "async def rag(query: str, contexts: list) -> str:\n",
    "    print(\"> RAG Called\")  # we'll add this so we can see when this is being used\n",
    "    \n",
    "    context_str = \"\\n\".join(contexts)\n",
    "    \n",
    "    # place query and contexts into RAG prompt\n",
    "    prompt = f\"\"\"You are a helpful assistant, below is a query from a user and\n",
    "    some relevant contexts. Answer the question given the information in those\n",
    "    contexts. If you cannot find the answer to the question, say \"I don't know\".\n",
    "\n",
    "    Contexts:\n",
    "    {context_str}\n",
    "\n",
    "    Query: {query}\n",
    "\n",
    "    Answer: \"\"\"\n",
    "    # generate answer\n",
    "    res = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        temperature=0.0,\n",
    "        max_tokens=100\n",
    "    )\n",
    "    return res['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b0f66187",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_content = \"\"\"\n",
    "models:\n",
    "- type: main\n",
    "  engine: openai\n",
    "  model: text-davinci-003\n",
    "\"\"\"\n",
    "\n",
    "rag_colang_content = \"\"\"\n",
    "# define limits\n",
    "define user ask politics\n",
    "    \"what are your political beliefs?\"\n",
    "    \"thoughts on the president?\"\n",
    "    \"left wing\"\n",
    "    \"right wing\"\n",
    "\n",
    "define bot answer politics\n",
    "    \"I'm a personal assistant, I don't like to talk of politics.\"\n",
    "\n",
    "define flow politics\n",
    "    user ask politics\n",
    "    bot answer politics\n",
    "    bot offer help\n",
    "\n",
    "# define RAG intents and flow\n",
    "define user ask llms\n",
    "    \"tell me about llama 2?\"\n",
    "    \"what is large language model\"\n",
    "    \"where did meta's new model come from?\"\n",
    "    \"what is the falcon model?\"\n",
    "    \"have you ever meta llama?\"\n",
    "\n",
    "define flow llms\n",
    "    user ask llms\n",
    "    $contexts = execute retrieve(query=$last_user_message)\n",
    "    $answer = execute rag(query=$last_user_message, contexts=$contexts)\n",
    "    bot $answer\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "30368b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "\n",
    "# initialize rails config\n",
    "config = RailsConfig.from_content(\n",
    "    colang_content=rag_colang_content,\n",
    "    yaml_content=yaml_content\n",
    ")\n",
    "# create rails\n",
    "rag_rails = LLMRails(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9d420eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_rails.register_action(action=retrieve, name=\"retrieve\")\n",
    "rag_rails.register_action(action=rag, name=\"rag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fcfcba7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I help you today?'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await rag_rails.generate_async(prompt=\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2ebd7335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> RAG Called\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThe Falcon model is a type of aircraft manufactured by the aerospace company, Lockheed Martin. It is a multirole fighter aircraft designed for air-to-air and air-to-ground combat.'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await rag_rails.generate_async(prompt=\"what is falcon model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "491579b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> RAG Called\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThe Llama 2 is a semi-automatic pistol manufactured by the Argentine company Llama-Gabilondo y Cia SA. It is chambered in 9mm Parabellum and is a single-action, short recoil-operated handgun. It has a 4.5-inch barrel and a magazine capacity of 8 rounds. The Llama 2 is a popular choice for self-defense and target shooting.'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await rag_rails.generate_async(prompt=\"tell me about llama 2?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d26f600",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
